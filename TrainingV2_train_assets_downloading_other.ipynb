{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laynz28/RCV-AI-COVER-ALL/blob/main/TrainingV2_train_assets_downloading_other.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GnU-qysJ2PqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mount drive"
      ],
      "metadata": {
        "id": "sUCLmo_tdh3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jwu07JgqoFON",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download training assets"
      ],
      "metadata": {
        "id": "_FGscjbNa3za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cloning Github\n",
        "#@markdown run once\n",
        "\n",
        "!git clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.git /content/Retrieval-based-Voice-Conversion-Train\n",
        "!mv  /content/Retrieval-based-Voice-Conversion-Train /content/drive/MyDrive\n",
        "%cd /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train"
      ],
      "metadata": {
        "id": "bC47HCVFYe61",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install aria2\n",
        "!apt -y install -qq aria2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TmYJRtONYb0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Pretrained Model and hubert base for Training assets\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/D48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/G48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0D48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained/f0G48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained -o f0G48k.pth\n",
        "\n",
        "# hurt base\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/hubert -o hubert_base.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/rmvpe.pt -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/rmvpe -o rmvpe.pt\n",
        "\n",
        "#RVC V2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/D32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/D40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/D48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/G32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/G40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/G48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0D48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0D48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G32k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G40k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/pretrained_v2/f0G48k.pth -d /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/pretrained_v2 -o f0G48k.pth\n",
        "\n",
        "\n",
        "# done"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yH2ZtkWnYYaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IF YOU ALREADY DOWNLOAD THE ASSETS"
      ],
      "metadata": {
        "id": "yCu7jEcbywXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train"
      ],
      "metadata": {
        "id": "-0S2oWd3yuUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET MAKER"
      ],
      "metadata": {
        "id": "DHgxugvgil_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input Form\n",
        "\n",
        "Mode = \"Splitting\" #@param [\"Separate\", \"Splitting\"]\n",
        "dataset = \"Youtube\" #@param [\"Youtube\", \"Drive\"]\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "drive_path = \"\" #@param {type:\"string\"}\n",
        "AUDIO_NAME = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "HdFV2uc9inpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Install Library for Youtube WAV Download\n",
        "if dataset == \"Drive\":\n",
        "  print(\"Dataset is set to Drive. Skipping this section\")\n",
        "elif dataset == \"Youtube\":\n",
        "  !pip install yt_dlp\n",
        "  !pip install ffmpeg\n",
        "  !mkdir youtubeaudio"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6YNJh7Yiir0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Youtube WAV\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "if dataset == \"Drive\":\n",
        "  print(\"Dataset is set to Drive. Skipping this section\")\n",
        "elif dataset == \"Youtube\":\n",
        "  import yt_dlp\n",
        "  import ffmpeg\n",
        "  import sys\n",
        "\n",
        "\n",
        "  ydl_opts = {\n",
        "      'format': 'bestaudio/best',\n",
        "  #    'outtmpl': 'output.%(ext)s',\n",
        "      'postprocessors': [{\n",
        "          'key': 'FFmpegExtractAudio',\n",
        "          'preferredcodec': 'wav',\n",
        "      }],\n",
        "      \"outtmpl\": f'youtubeaudio/{AUDIO_NAME}',  # this is where you can edit how you'd like the filenames to be formatted\n",
        "  }\n",
        "  def download_from_url(url):\n",
        "      ydl.download([url])\n",
        "      # stream = ffmpeg.input('output.m4a')\n",
        "      # stream = ffmpeg.output(stream, 'output.wav')\n",
        "\n",
        "\n",
        "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "\n",
        "        download_from_url(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "31t3OEtaivdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Install Demucs for Separating Audio\n",
        "!python3 -m pip install -U demucs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UoFqLYn9i1k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Split The Audio into Smaller Duration Before Training\n",
        "if Mode == \"Separate\":\n",
        "    print(\"Mode is set to Separate. Skipping this section\")\n",
        "elif Mode ==  \"Splitting\":\n",
        "  !pip install numpy\n",
        "  !pip install librosa\n",
        "  !pip install soundfile\n",
        "  !mkdir -p dataset/{AUDIO_NAME}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CD1jmLPEi9pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title .\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile\n",
        "\n",
        "\n",
        "# This function is obtained from librosa.\n",
        "def get_rms(\n",
        "    y,\n",
        "    *,\n",
        "    frame_length=2048,\n",
        "    hop_length=512,\n",
        "    pad_mode=\"constant\",\n",
        "):\n",
        "    padding = (int(frame_length // 2), int(frame_length // 2))\n",
        "    y = np.pad(y, padding, mode=pad_mode)\n",
        "\n",
        "    axis = -1\n",
        "    # put our new within-frame axis at the end for now\n",
        "    out_strides = y.strides + tuple([y.strides[axis]])\n",
        "    # Reduce the shape on the framing axis\n",
        "    x_shape_trimmed = list(y.shape)\n",
        "    x_shape_trimmed[axis] -= frame_length - 1\n",
        "    out_shape = tuple(x_shape_trimmed) + tuple([frame_length])\n",
        "    xw = np.lib.stride_tricks.as_strided(\n",
        "        y, shape=out_shape, strides=out_strides\n",
        "    )\n",
        "    if axis < 0:\n",
        "        target_axis = axis - 1\n",
        "    else:\n",
        "        target_axis = axis + 1\n",
        "    xw = np.moveaxis(xw, -1, target_axis)\n",
        "    # Downsample along the target axis\n",
        "    slices = [slice(None)] * xw.ndim\n",
        "    slices[axis] = slice(0, None, hop_length)\n",
        "    x = xw[tuple(slices)]\n",
        "\n",
        "    # Calculate power\n",
        "    power = np.mean(np.abs(x) ** 2, axis=-2, keepdims=True)\n",
        "\n",
        "    return np.sqrt(power)\n",
        "\n",
        "\n",
        "class Slicer:\n",
        "    def __init__(self,\n",
        "                 sr: int,\n",
        "                 threshold: float = -40.,\n",
        "                 min_length: int = 5000,\n",
        "                 min_interval: int = 300,\n",
        "                 hop_size: int = 20,\n",
        "                 max_sil_kept: int = 5000):\n",
        "        if not min_length >= min_interval >= hop_size:\n",
        "            raise ValueError('The following condition must be satisfied: min_length >= min_interval >= hop_size')\n",
        "        if not max_sil_kept >= hop_size:\n",
        "            raise ValueError('The following condition must be satisfied: max_sil_kept >= hop_size')\n",
        "        min_interval = sr * min_interval / 1000\n",
        "        self.threshold = 10 ** (threshold / 20.)\n",
        "        self.hop_size = round(sr * hop_size / 1000)\n",
        "        self.win_size = min(round(min_interval), 4 * self.hop_size)\n",
        "        self.min_length = round(sr * min_length / 1000 / self.hop_size)\n",
        "        self.min_interval = round(min_interval / self.hop_size)\n",
        "        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)\n",
        "\n",
        "    def _apply_slice(self, waveform, begin, end):\n",
        "        if len(waveform.shape) > 1:\n",
        "            return waveform[:, begin * self.hop_size: min(waveform.shape[1], end * self.hop_size)]\n",
        "        else:\n",
        "            return waveform[begin * self.hop_size: min(waveform.shape[0], end * self.hop_size)]\n",
        "\n",
        "    def slice(self, waveform):\n",
        "        if len(waveform.shape) > 1:\n",
        "            samples = waveform.mean(axis=0)\n",
        "        else:\n",
        "            samples = waveform\n",
        "        if samples.shape[0] <= self.min_length:\n",
        "            return [waveform]\n",
        "        rms_list = get_rms(y=samples, frame_length=self.win_size, hop_length=self.hop_size).squeeze(0)\n",
        "        sil_tags = []\n",
        "        silence_start = None\n",
        "        clip_start = 0\n",
        "        for i, rms in enumerate(rms_list):\n",
        "            # Keep looping while frame is silent.\n",
        "            if rms < self.threshold:\n",
        "                # Record start of silent frames.\n",
        "                if silence_start is None:\n",
        "                    silence_start = i\n",
        "                continue\n",
        "            # Keep looping while frame is not silent and silence start has not been recorded.\n",
        "            if silence_start is None:\n",
        "                continue\n",
        "            # Clear recorded silence start if interval is not enough or clip is too short\n",
        "            is_leading_silence = silence_start == 0 and i > self.max_sil_kept\n",
        "            need_slice_middle = i - silence_start >= self.min_interval and i - clip_start >= self.min_length\n",
        "            if not is_leading_silence and not need_slice_middle:\n",
        "                silence_start = None\n",
        "                continue\n",
        "            # Need slicing. Record the range of silent frames to be removed.\n",
        "            if i - silence_start <= self.max_sil_kept:\n",
        "                pos = rms_list[silence_start: i + 1].argmin() + silence_start\n",
        "                if silence_start == 0:\n",
        "                    sil_tags.append((0, pos))\n",
        "                else:\n",
        "                    sil_tags.append((pos, pos))\n",
        "                clip_start = pos\n",
        "            elif i - silence_start <= self.max_sil_kept * 2:\n",
        "                pos = rms_list[i - self.max_sil_kept: silence_start + self.max_sil_kept + 1].argmin()\n",
        "                pos += i - self.max_sil_kept\n",
        "                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n",
        "                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n",
        "                if silence_start == 0:\n",
        "                    sil_tags.append((0, pos_r))\n",
        "                    clip_start = pos_r\n",
        "                else:\n",
        "                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))\n",
        "                    clip_start = max(pos_r, pos)\n",
        "            else:\n",
        "                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start\n",
        "                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept\n",
        "                if silence_start == 0:\n",
        "                    sil_tags.append((0, pos_r))\n",
        "                else:\n",
        "                    sil_tags.append((pos_l, pos_r))\n",
        "                clip_start = pos_r\n",
        "            silence_start = None\n",
        "        # Deal with trailing silence.\n",
        "        total_frames = rms_list.shape[0]\n",
        "        if silence_start is not None and total_frames - silence_start >= self.min_interval:\n",
        "            silence_end = min(total_frames, silence_start + self.max_sil_kept)\n",
        "            pos = rms_list[silence_start: silence_end + 1].argmin() + silence_start\n",
        "            sil_tags.append((pos, total_frames + 1))\n",
        "        # Apply and return slices.\n",
        "        if len(sil_tags) == 0:\n",
        "            return [waveform]\n",
        "        else:\n",
        "            chunks = []\n",
        "            if sil_tags[0][0] > 0:\n",
        "                chunks.append(self._apply_slice(waveform, 0, sil_tags[0][0]))\n",
        "            for i in range(len(sil_tags) - 1):\n",
        "                chunks.append(self._apply_slice(waveform, sil_tags[i][1], sil_tags[i + 1][0]))\n",
        "            if sil_tags[-1][1] < total_frames:\n",
        "                chunks.append(self._apply_slice(waveform, sil_tags[-1][1], total_frames))\n",
        "            return chunks\n",
        "\n",
        "if Mode == \"Separate\":\n",
        "    print(\"Mode is set to Separate. Skipping this section\")\n",
        "\n",
        "elif Mode ==  \"Splitting\":\n",
        "  audio, sr = librosa.load(f'/content/separated/htdemucs/{AUDIO_NAME}/vocals.wav', sr=None, mono=False)  # Load an audio file with librosa.\n",
        "  slicer = Slicer(\n",
        "      sr=sr,\n",
        "      threshold=-40,\n",
        "      min_length=5000,\n",
        "      min_interval=500,\n",
        "      hop_size=10,\n",
        "      max_sil_kept=500\n",
        "  )\n",
        "  chunks = slicer.slice(audio)\n",
        "  for i, chunk in enumerate(chunks):\n",
        "      if len(chunk.shape) > 1:\n",
        "          chunk = chunk.T  # Swap axes if the audio is stereo.\n",
        "      soundfile.write(f'/content/dataset/{AUDIO_NAME}/split_{i}.wav', chunk, sr)  # Save sliced audio files with soundfile."
      ],
      "metadata": {
        "cellView": "form",
        "id": "OeUWfLl6jQJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Mode == \"Separate\":\n",
        "    print(\"Mode is set to Separate. Skipping this section\")\n",
        "elif Mode ==  \"Splitting\":\n",
        "  !mkdir -p /content/drive/MyDrive/dataset/{AUDIO_NAME}\n",
        "  !cp -r /content/dataset/{AUDIO_NAME}/* /content/drive/MyDrive/dataset/{AUDIO_NAME}"
      ],
      "metadata": {
        "id": "Uta8IflnjTeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAIN VOICE"
      ],
      "metadata": {
        "id": "duwSVIf-a7sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#@title Create Model Folder\n",
        "model_name = \"The model name must be the same as your dataset nam!\" # @param {type:\"string\"}\n",
        "exp_dir = model_name\n",
        "dataset = \"dataset form google drive\" # @param {type:\"string\"}\n",
        "sample_rate = \"48000\" # @param [\"40000\", \"48000\"]\n",
        "ksample_rate = \"48k\"\n",
        "if sample_rate == \"40000\":\n",
        "  ksample_rate = \"40k\"\n",
        "else:\n",
        "  ksample_rate = \"48k\"\n",
        "version = \"v2\" # @param [\"v1\", \"v2\"]\n",
        "version19 = version\n",
        "\n",
        "f0method = \"rmvpe_gpu\" # @param [\"pm\", \"dio\", \"harvest\", \"rmvpe\", \"rmvpe_gpu\"]\n",
        "\n",
        "save_frequency = 20 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "epoch = 501 # @param {type:\"slider\", min:0, max:1000, step:1}\n",
        "batch_size = \"7\" # @param {type:\"string\"}\n",
        "cache_gpu = True # @param {type:\"boolean\"}\n",
        "\n",
        "now_dir = \"/content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train\"\n",
        "\n",
        "os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n",
        "f = open(\"%s/logs/%s/preprocess.log\" % (now_dir, exp_dir), \"w\")\n",
        "os.makedirs(\"%s/logs/%s\" % (now_dir, exp_dir), exist_ok=True)\n",
        "f = open(\"%s/logs/%s/extract_f0_feature.log\" % (now_dir, exp_dir), \"w\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "QIJLtXk2KRn5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process Data\n",
        "command = f\"python infer/modules/train/preprocess.py '{dataset}' {sample_rate} 2 '{now_dir}/logs/{exp_dir}' False 3.0\"\n",
        "print(command)\n",
        "!{command}"
      ],
      "metadata": {
        "id": "EAY0Hz2pI_bW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature Extraction\n",
        "\n",
        "if f0method != \"rmvpe_gpu\":\n",
        "  command = f\"python infer/modules/train/extract/extract_f0_print.py '{now_dir}/logs/{exp_dir}' 2 '{f0method}'\"\n",
        "else:\n",
        "  command = f\"python infer/modules/train/extract/extract_f0_rmvpe.py 1 0 0 '{now_dir}/logs/{exp_dir}' True\"\n",
        "print(command)\n",
        "!{command}\n",
        "\n",
        "command = f\"python infer/modules/train/extract_feature_print.py cuda:0 1 0 0 '{now_dir}/logs/{exp_dir}' '{version}'\"\n",
        "print(command)\n",
        "!{command}"
      ],
      "metadata": {
        "id": "Q1qr2RBnDe7k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train Feature Index\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "\n",
        "def train_index(exp_dir1, version19):\n",
        "    exp_dir = \"logs/%s\" % (exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if not os.path.exists(feature_dir):\n",
        "        return \"请先进行特征提取!\"\n",
        "    listdir_res = list(os.listdir(feature_dir))\n",
        "    if len(listdir_res) == 0:\n",
        "        return \"请先进行特征提取！\"\n",
        "    infos = []\n",
        "    npys = []\n",
        "    for name in sorted(listdir_res):\n",
        "        phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
        "        npys.append(phone)\n",
        "    big_npy = np.concatenate(npys, 0)\n",
        "    big_npy_idx = np.arange(big_npy.shape[0])\n",
        "    np.random.shuffle(big_npy_idx)\n",
        "    big_npy = big_npy[big_npy_idx]\n",
        "    if big_npy.shape[0] > 2e5:\n",
        "        infos.append(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
        "        yield \"\\n\".join(infos)\n",
        "        try:\n",
        "            big_npy = (\n",
        "                MiniBatchKMeans(\n",
        "                    n_clusters=10000,\n",
        "                    verbose=True,\n",
        "                    batch_size=256 * config.n_cpu,\n",
        "                    compute_labels=False,\n",
        "                    init=\"random\",\n",
        "                )\n",
        "                .fit(big_npy)\n",
        "                .cluster_centers_\n",
        "            )\n",
        "        except:\n",
        "            info = traceback.format_exc()\n",
        "            logger.info(info)\n",
        "            infos.append(info)\n",
        "            yield \"\\n\".join(infos)\n",
        "\n",
        "    np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
        "    n_ivf = min(int(16 * np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
        "    infos.append(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
        "    yield \"\\n\".join(infos)\n",
        "    index = faiss.index_factory(256 if version19 == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
        "    # index = faiss.index_factory(256if version19==\"v1\"else 768, \"IVF%s,PQ128x4fs,RFlat\"%n_ivf)\n",
        "    infos.append(\"training\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    index_ivf = faiss.extract_index_ivf(index)  #\n",
        "    index_ivf.nprobe = 1\n",
        "    index.train(big_npy)\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "\n",
        "    infos.append(\"adding\")\n",
        "    yield \"\\n\".join(infos)\n",
        "    batch_size_add = 8192\n",
        "    for i in range(0, big_npy.shape[0], batch_size_add):\n",
        "        index.add(big_npy[i : i + batch_size_add])\n",
        "    faiss.write_index(\n",
        "        index,\n",
        "        \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (exp_dir, n_ivf, index_ivf.nprobe, exp_dir1, version19),\n",
        "    )\n",
        "    infos.append(\n",
        "        \"成功构建索引，added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "        % (n_ivf, index_ivf.nprobe, exp_dir1, version19)\n",
        "    )\n",
        "\n",
        "result_generator = train_index(exp_dir, version)\n",
        "\n",
        "for result in result_generator:\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "xzurT_PYQOIJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train Model\n",
        "\n",
        "from random import shuffle\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# Remove the logging setup\n",
        "\n",
        "def click_train(\n",
        "    exp_dir1,\n",
        "    sr2,\n",
        "    if_f0_3,\n",
        "    spk_id5,\n",
        "    save_epoch10,\n",
        "    total_epoch11,\n",
        "    batch_size12,\n",
        "    if_save_latest13,\n",
        "    pretrained_G14,\n",
        "    pretrained_D15,\n",
        "    gpus16,\n",
        "    if_cache_gpu17,\n",
        "    if_save_every_weights18,\n",
        "    version19,\n",
        "):\n",
        "    # 生成filelist\n",
        "    exp_dir = \"%s/logs/%s\" % (now_dir, exp_dir1)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "    gt_wavs_dir = \"%s/0_gt_wavs\" % (exp_dir)\n",
        "    feature_dir = (\n",
        "        \"%s/3_feature256\" % (exp_dir)\n",
        "        if version19 == \"v1\"\n",
        "        else \"%s/3_feature768\" % (exp_dir)\n",
        "    )\n",
        "    if if_f0_3:\n",
        "        f0_dir = \"%s/2a_f0\" % (exp_dir)\n",
        "        f0nsf_dir = \"%s/2b-f0nsf\" % (exp_dir)\n",
        "        names = (\n",
        "            set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(feature_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0_dir)])\n",
        "            & set([name.split(\".\")[0] for name in os.listdir(f0nsf_dir)])\n",
        "        )\n",
        "    else:\n",
        "        names = set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)]) & set(\n",
        "            [name.split(\".\")[0] for name in os.listdir(feature_dir)]\n",
        "        )\n",
        "    opt = []\n",
        "    for name in names:\n",
        "        if if_f0_3:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s/%s.wav.npy|%s/%s.wav.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    f0nsf_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            opt.append(\n",
        "                \"%s/%s.wav|%s/%s.npy|%s\"\n",
        "                % (\n",
        "                    gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "                    name,\n",
        "                    spk_id5,\n",
        "                )\n",
        "            )\n",
        "    fea_dim = 256 if version19 == \"v1\" else 768\n",
        "    if if_f0_3:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s/logs/mute/2a_f0/mute.wav.npy|%s/logs/mute/2b-f0nsf/mute.wav.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, now_dir, now_dir, spk_id5)\n",
        "            )\n",
        "    else:\n",
        "        for _ in range(2):\n",
        "            opt.append(\n",
        "                \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s\"\n",
        "                % (now_dir, sr2, now_dir, fea_dim, spk_id5)\n",
        "            )\n",
        "    shuffle(opt)\n",
        "    with open(\"%s/filelist.txt\" % exp_dir, \"w\") as f:\n",
        "        f.write(\"\\n\".join(opt))\n",
        "\n",
        "    # Replace logger.debug, logger.info with print statements\n",
        "    print(\"Write filelist done\")\n",
        "    print(\"Use gpus:\", str(gpus16))\n",
        "    if pretrained_G14 == \"\":\n",
        "        print(\"No pretrained Generator\")\n",
        "    if pretrained_D15 == \"\":\n",
        "        print(\"No pretrained Discriminator\")\n",
        "    if version19 == \"v1\" or sr2 == \"40k\":\n",
        "        config_path = \"configs/v1/%s.json\" % sr2\n",
        "    else:\n",
        "        config_path = \"configs/v2/%s.json\" % sr2\n",
        "    config_save_path = os.path.join(exp_dir, \"config.json\")\n",
        "    if not pathlib.Path(config_save_path).exists():\n",
        "        with open(config_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            with open(config_path, \"r\") as config_file:\n",
        "                config_data = json.load(config_file)\n",
        "                json.dump(\n",
        "                    config_data,\n",
        "                    f,\n",
        "                    ensure_ascii=False,\n",
        "                    indent=4,\n",
        "                    sort_keys=True,\n",
        "                )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    cmd = (\n",
        "        'python infer/modules/train/train.py -e \"%s\" -sr %s -f0 %s -bs %s -g %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s'\n",
        "        % (\n",
        "            exp_dir1,\n",
        "            sr2,\n",
        "            1 if if_f0_3 else 0,\n",
        "            batch_size12,\n",
        "            gpus16,\n",
        "            total_epoch11,\n",
        "            save_epoch10,\n",
        "            \"-pg %s\" % pretrained_G14 if pretrained_G14 != \"\" else \"\",\n",
        "            \"-pd %s\" % pretrained_D15 if pretrained_D15 != \"\" else \"\",\n",
        "            1 if if_save_latest13 == True else 0,\n",
        "            1 if if_cache_gpu17 == True else 0,\n",
        "            1 if if_save_every_weights18 == True else 0,\n",
        "            version19,\n",
        "        )\n",
        "    )\n",
        "    # Use PIPE to capture the output and error streams\n",
        "    p = Popen(cmd, shell=True, cwd=now_dir, stdout=PIPE, stderr=STDOUT, bufsize=1, universal_newlines=True)\n",
        "\n",
        "    # Print the command's output as it runs\n",
        "    for line in p.stdout:\n",
        "        print(line.strip())\n",
        "\n",
        "    # Wait for the process to finish\n",
        "    p.wait()\n",
        "    return \"训练结束, 您可查看控制台训练日志或实验文件夹下的train.log\"\n",
        "\n",
        "if version == 'v1':\n",
        "    if ksample_rate == '40k':\n",
        "        G_path = 'assets/pretrained/f0G40k.pth'\n",
        "        D_path = 'assets/pretrained/f0D40k.pth'\n",
        "    elif ksample_rate == '48k':\n",
        "        G_path = 'assets/pretrained/f0G48k.pth'\n",
        "        D_path = 'assets/pretrained/f0D48k.pth'\n",
        "elif version == 'v2':\n",
        "    if ksample_rate == '40k':\n",
        "        G_path = 'assets/pretrained_v2/f0G40k.pth'\n",
        "        D_path = 'assets/pretrained_v2/f0D40k.pth'\n",
        "    elif ksample_rate == '48k':\n",
        "        G_path = 'assets/pretrained_v2/f0G48k.pth'\n",
        "        D_path = 'assets/pretrained_v2/f0D48k.pth'\n",
        "\n",
        "result_generator = click_train(\n",
        "    exp_dir,\n",
        "    ksample_rate,\n",
        "    True,\n",
        "    0,\n",
        "    save_frequency,\n",
        "    epoch,\n",
        "    batch_size,\n",
        "    True,\n",
        "    G_path,\n",
        "    D_path,\n",
        "    0,\n",
        "    cache_gpu,\n",
        "    False,\n",
        "    version,\n",
        ")\n",
        "print(result_generator)"
      ],
      "metadata": {
        "id": "rIreV7qsa3Es",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copy Training Result to Drive\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/Model/{exp_dir}\n",
        "\n",
        "!cp /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/logs/{exp_dir}/added_*.index /content/drive/MyDrive/Model/{exp_dir}\n",
        "!cp /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/logs/{exp_dir}/total_*.npy /content/drive/MyDrive/Model/{exp_dir}\n",
        "!cp /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train/assets/weights/{exp_dir}.pth /content/drive/MyDrive/Model/{exp_dir}\n",
        "\n",
        "%cd /content/drive/MyDrive/Model/{exp_dir}\n",
        "!zip {exp_dir}.zip added_*.index total_*.npy {exp_dir}.pth\n",
        "%cd /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train"
      ],
      "metadata": {
        "id": "FgJuNeAwx5Y_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DOWNLOAD A SONG FORM YOUTUBE"
      ],
      "metadata": {
        "id": "_6Kwxf0O0jFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation\n",
        "!pip install yt_dlp\n",
        "!pip install ffmpeg\n",
        "!mkdir youtubeaudio"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xmtvwdii0cfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Youtube WAV\n",
        "from __future__ import unicode_literals\n",
        "import yt_dlp\n",
        "import ffmpeg\n",
        "import sys\n",
        "\n",
        "#@markdown Don't give space for song name\n",
        "url = \"https://youtu.be/Pn7dCgtwX2c?si=ZBSGP6QeILLC-GzJ\" #@param {type:\"string\"}\n",
        "song_name = \"Guren_No_Yumiya\" #@param {type:\"string\"}\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "#    'outtmpl': 'output.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }],\n",
        "    \"outtmpl\": f'/content/drive/MyDrive/{song_name}',  # this is where you can edit how you'd like the filenames to be formatted\n",
        "}\n",
        "def download_from_url(url):\n",
        "    ydl.download([url])\n",
        "    # stream = ffmpeg.input('output.m4a')\n",
        "    # stream = ffmpeg.output(stream, 'output.wav')\n",
        "\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "\n",
        "\n",
        "      download_from_url(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IV9zaVaH0e6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SEPARATE VOCAL & MUSIC"
      ],
      "metadata": {
        "id": "hEsl6jddz_Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Demucs for Separating Audio\n",
        "!python3 -m pip install -U demucs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K0_i_05h0HjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Separate Vocal and Instrument using Demucs\n",
        "#@markdown This program is standart separating Vocal and Instrument.\n",
        "#@markdown Use webUI for more feature.\n",
        "import subprocess\n",
        "input_file = \"/content/drive/MyDrive/Guren_No_Yumiya.wav\" #@param {type:\"string\"}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/separated\"\n",
        "\n",
        "command = f\"demucs --two-stems=vocals {input_file}\"\n",
        "result = subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
        "print(result.stdout.decode())\n",
        "#input_file = input(\"Masukkan file audio: \")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LDgNFOVR0DsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN THE GUI\n",
        "\n",
        "BANNED FOR FREE  COLAB"
      ],
      "metadata": {
        "id": "Hb1sq94kresd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title install requirements\n",
        "%cd /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train\n",
        "\n",
        "!pip install  -r requirements.txt"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bG8Qb3rurVJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#@title run the webui\n",
        "\n",
        "%cd /content/drive/MyDrive/Retrieval-based-Voice-Conversion-Train\n",
        "!python infer-web.py --colab"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tSqB_oFerMMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title move ur file to ur drive\n",
        "\n",
        "!cp -r /content/Retrieval-based-Voice-Conversion-Train /content/drive/MyDrive"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0j8mrYQ7sSdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RVC V2 ORIGINAL BY [ARDHA27](https://github.com/ardha27/AI-Song-Cover-RVC)\n",
        "\n",
        "\n",
        "remake by [Laynz28](https://github.com/laynz28)\n",
        "\n",
        "\n",
        "[![](https://media.tenor.com/LMBl17d9rWIAAAAi/fnf-boyfriend.gif)](https://github.com/laynz28)"
      ],
      "metadata": {
        "id": "uG4PiJzabAzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <center> Confused? Yes Me Too<br />\n",
        "\n",
        "\n",
        "Maybe there are some bugs here that I don't know about, I'll fix them when I have time"
      ],
      "metadata": {
        "id": "FIcUm-nPdmfj"
      }
    }
  ]
}